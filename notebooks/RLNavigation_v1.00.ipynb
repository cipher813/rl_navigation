{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"/Volumes/BC_Clutch/Dropbox/DeepRLND/rl_navigation/\"\n",
    "APP_PATH = PATH + \"data/Banana.app\"\n",
    "CHART_PATH = PATH + \"charts/\"\n",
    "CHECKPOINT_PATH = PATH + \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# env = UnityEnvironment(APP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brain_name = env.brain_names[0]\n",
    "# brain = env.brains[brain_name]\n",
    "# env_info = env.reset(True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(f\"Number of agents: {len(env_info.agents)}\")\n",
    "# print(f\"Number of actions: {brain.vector_action_space_size}\")\n",
    "# print(f\"Number of states: {len(env_info.vector_observations[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "GAMMA = 0.97\n",
    "TAU = 1e-3\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64\n",
    "SEED = 0\n",
    "UPDATE_EVERY = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, hidden=[64,64]):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0],hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Fixed-size buffer to store experience tuples.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\",field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "class Vanilla:\n",
    "    \"\"\"Initialize the base agent class.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed,buffer_size=BUFFER_SIZE,\n",
    "                 batch_size=BATCH_SIZE, gamma=GAMMA,lr=LR,update_every=UPDATE_EVERY):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.buffer_size = int(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.update_every = update_every\n",
    "        \n",
    "        # Q Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=LR)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # learn every UPDATE EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # if enough samples available in memory, get random subset and learn\n",
    "            if len(self.memory)>BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Return actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # sets the pytorch module into evaluation mode\n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # epsilon greedy action selection\n",
    "        if random.random()>eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value params given batch of experience tuples.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        # compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # minimize loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backwward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target,TAU)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        Soft update model params.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(),local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data+(1.0-tau)*target_param.data)\n",
    "            \n",
    "# class DeepQNetwork:\n",
    "#     \"\"\"Deep Q Learning Network.\"\"\"\n",
    "#     def __init__(self, PATH, model_name, n_episodes, max_t, eps_start, eps_end, eps_decay, seed):\n",
    "#         self.env = UnityEnvironment(PATH + \"data/Banana.app\")\n",
    "#         self.brain_name = self.env.brain_names[0]\n",
    "#         self.brain = self.env.brains[self.brain_name]\n",
    "#         self.env_info = self.env.reset(True)[self.brain_name]\n",
    "#         self.scores = []\n",
    "#         self.scores_window = deque(maxlen=100)\n",
    "#         self.eps = eps_start\n",
    "#         self.state = self.env_info.vector_observations[0]\n",
    "#         self.state_size = len(self.state)\n",
    "#         self.action_size = self.brain.vector_action_space_size\n",
    "#         self.seed = seed\n",
    "#         self.agent = Vanilla(self.state_size, self.action_size, self.seed)\n",
    "#         self.timestamp = re.sub(r\"\\D\",\"\",str(datetime.datetime.now()))[:12]\n",
    "        \n",
    "#     def train(self, n_episodes, max_t, state_size, action_size, seed):\n",
    "#         start = time.time()\n",
    "#         for i_episode in range(1, n_episodes+1):\n",
    "#             state = self.env.reset()\n",
    "#             score = 0\n",
    "#             for t in range(max_t):\n",
    "#                 state = self.env_info.vector_observations[0]\n",
    "#                 action = self.agent.act(self.state,self.eps)\n",
    "#                 env_info = self.env.step(action)[self.brain_name]\n",
    "#                 next_state = self.env_info.vector_observations[0]\n",
    "#                 reward = self.env_info.rewards[0]\n",
    "#                 done = self.env_info.local_done[0]\n",
    "#                 self.agent.step(state, action, reward, next_state, done)\n",
    "#                 score += reward\n",
    "#                 state = next_state\n",
    "#                 if done:\n",
    "#                     break\n",
    "#             scores_window.append(score)\n",
    "#             scores.append(score)\n",
    "#             self.eps = max(eps_end, eps_decay*eps)\n",
    "#             print(f\"Episode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\")\n",
    "#             if i_episode % 100 == 0:\n",
    "#                 print(f\"Episode {i_episode}\\tAverage Score {np.mean(scores_window):.2f}\")\n",
    "#             if np.mean(scores_window)>=score_target:\n",
    "#                 print(f\"Environment solved in {i_episode:d} episodes.\\tAverage Score: {np.mean(scores_window):.2f}\")\n",
    "#                 checkpath = PATH + f\"models/checkpoint-{model_name}-{timestamp}.pth\"\n",
    "#                 torch.save(agent.qnetwork_local.state_dict(),checkpath)\n",
    "#                 print(f\"Checkpoint saved at {checkpath}\")\n",
    "#                 break\n",
    "#         end = time.time()\n",
    "#         result_dict = {\n",
    "#                         \"scores\":scores,\n",
    "#                         \"clocktime\":round((end-start)/60,2),\n",
    "#                         \"state_size\":state_size,\n",
    "#                         \"action_size\":action_size,\n",
    "#                         \"seed\":seed,\n",
    "#                         \"n_episodes\":n_episodes,\n",
    "#                         \"max_t\":max_t,\n",
    "#                         \"eps_start\":eps_start,\n",
    "#                         \"eps_end\":eps_end,\n",
    "#                         \"eps_decay\":eps_decay,\n",
    "#                         \"train_mode\":train_mode\n",
    "#                         }\n",
    "#         pklpath = PATH + f\"charts/ResultDict-{timestamp}.pkl\"\n",
    "#         with open(pklpath,'wb') as handle:\n",
    "#             pickle.dump(result_dict,handle)\n",
    "#         print(f\"Scores pickled at {pklpath}\")\n",
    "#         return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dqn(model_name, n_episodes, max_t, eps_start, eps_end, eps_decay,train_mode):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    env_info = env.reset(train_mode)[brain_name]\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start\n",
    "    print(\"Loading monkey.\")\n",
    "    print(\"Monkey training on bananas.\")\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            state = env_info.vector_observations[0]            # get the current state\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward                                # update the score\n",
    "            state = next_state                             # roll over the state to next time step\n",
    "            if done:                                       # exit loop if episode finished\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=score_target:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            checkpath = CHECKPOINT_PATH + f'checkpoint-{model_name}-{timestamp}.pth'\n",
    "            torch.save(agent.qnetwork_local.state_dict(), checkpath)\n",
    "            print(f\"Checkpoint saved at {checkpath}\")\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "model_name = \"Vanilla\"\n",
    "n_episodes = 1000\n",
    "max_t = 1000\n",
    "eps_start = 0.4\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.995\n",
    "state_size = 37\n",
    "action_size = 4\n",
    "seed = 0\n",
    "train_mode = True\n",
    "\n",
    "score_target = 0.0\n",
    "\n",
    "timestamp = re.sub(r\"\\D\",\"\",str(datetime.datetime.now()))[:12]\n",
    "\n",
    "env = UnityEnvironment(file_name=APP_PATH)\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset()[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like:\n",
      " [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:\\n', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agent = Vanilla(state_size, action_size, seed)\n",
    "# scores = dqn(model_name, n_episodes, max_t, eps_start, eps_end, eps_decay,train_mode=True)\n",
    "\n",
    "# timestamp = re.sub(r\"\\D\",\"\",str(datetime.datetime.now()))[:12]\n",
    "\n",
    "# dqn = DeepQNetwork(PATH, model_name, n_episodes, max_t, eps_start, eps_end, eps_decay, seed)\n",
    "\n",
    "# result_dict = dqn.train(n_episodes, max_t, state_size, action_size, seed)\n",
    "\n",
    "# result_dict = DeepQNetwork.train(n_episodes, max_t, state_size, action_size, seed, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Vanilla**\n",
      "Loading monkey.\n",
      "Monkey training on bananas.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0f1230b234f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"**{name}**\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     result_dict[name] = {\n",
      "\u001b[0;32m<ipython-input-8-2de434e315da>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(model_name, n_episodes, max_t, eps_start, eps_end, eps_decay, train_mode)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m                                \u001b[0;31m# update the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m                             \u001b[0;31m# roll over the state to next time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a5f96a43ecf7>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a5f96a43ecf7>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mQ_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# get expected Q values from local model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mQ_expected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_expected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "# train agent\n",
    "result_dict = {}\n",
    "function_list = [\n",
    "                 (\"Vanilla\",Vanilla(state_size, action_size, seed)),\n",
    "#                  (\"Double\", Double(state_size, action_size, seed)),\n",
    "#                  (\"PrioritizedReplay\",PrioritizedReplay(state_size, action_size, seed))\n",
    "                ]\n",
    "\n",
    "for function in function_list:\n",
    "    start = time.time()\n",
    "    name = function[0]\n",
    "    agent = function[1]\n",
    "    print(f\"**{name}**\")\n",
    "    scores = dqn(name, n_episodes, max_t, eps_start, eps_end, eps_decay,train_mode)\n",
    "    end = time.time()\n",
    "    result_dict[name] = {\n",
    "                    \"scores\":scores,\n",
    "                    \"clocktime\":round((end-start)/60,2),\n",
    "                    \"state_size\":state_size,\n",
    "                    \"action_size\":action_size,\n",
    "                    \"seed\":seed,\n",
    "                    \"n_episodes\":n_episodes,\n",
    "                    \"max_t\":max_t,\n",
    "                    \"eps_start\":eps_start,\n",
    "                    \"eps_end\":eps_end,\n",
    "                    \"eps_decay\":eps_decay,\n",
    "                    \"train_mode\":train_mode\n",
    "                    }\n",
    "\n",
    "pklpath = CHART_PATH + f\"ResultDict-{timestamp}.pkl\"\n",
    "with open(pklpath, 'wb') as handle:\n",
    "    pickle.dump(result_dict, handle)\n",
    "print(f\"Scores pickled at {pklpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
